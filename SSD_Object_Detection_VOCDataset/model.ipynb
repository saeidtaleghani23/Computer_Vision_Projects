{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download VOC 2012 dataset using torch\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # create dataset folder\n",
    "    root= './dataset'\n",
    "    if not os.path.exists(root):\n",
    "        os.makedirs(root)\n",
    "    torchvision.datasets.VOCDetection(\n",
    "    root=root,\n",
    "    year = '2012',\n",
    "    image_set = 'trainval',\n",
    "    download=True,\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "summary(vgg16, input_size=(3, 300, 300), device= 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 300, 300]           1,792\n",
      "              ReLU-2         [-1, 64, 300, 300]               0\n",
      "            Conv2d-3         [-1, 64, 300, 300]          36,928\n",
      "              ReLU-4         [-1, 64, 300, 300]               0\n",
      "         MaxPool2d-5         [-1, 64, 150, 150]               0\n",
      "            Conv2d-6        [-1, 128, 150, 150]          73,856\n",
      "              ReLU-7        [-1, 128, 150, 150]               0\n",
      "            Conv2d-8        [-1, 128, 150, 150]         147,584\n",
      "              ReLU-9        [-1, 128, 150, 150]               0\n",
      "        MaxPool2d-10          [-1, 128, 75, 75]               0\n",
      "           Conv2d-11          [-1, 256, 75, 75]         295,168\n",
      "             ReLU-12          [-1, 256, 75, 75]               0\n",
      "           Conv2d-13          [-1, 256, 75, 75]         590,080\n",
      "             ReLU-14          [-1, 256, 75, 75]               0\n",
      "           Conv2d-15          [-1, 256, 75, 75]         590,080\n",
      "             ReLU-16          [-1, 256, 75, 75]               0\n",
      "        MaxPool2d-17          [-1, 256, 38, 38]               0\n",
      "           Conv2d-18          [-1, 512, 38, 38]       1,180,160\n",
      "             ReLU-19          [-1, 512, 38, 38]               0\n",
      "           Conv2d-20          [-1, 512, 38, 38]       2,359,808\n",
      "             ReLU-21          [-1, 512, 38, 38]               0\n",
      "           Conv2d-22          [-1, 512, 38, 38]       2,359,808\n",
      "             ReLU-23          [-1, 512, 38, 38]               0\n",
      "================================================================\n",
      "Total params: 7,635,264\n",
      "Trainable params: 7,635,264\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 382.73\n",
      "Params size (MB): 29.13\n",
      "Estimated Total Size (MB): 412.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load imagenet pretrained vgg network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Load the pretrained VGG-16 model\n",
    "base_model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "            \n",
    "# Extract all maxpool layers' index\n",
    "max_pool_pos = [idx for idx, layer in enumerate(list(base_model.features)) if isinstance(layer, nn.MaxPool2d)]\n",
    "# force the output of conv4_3 to have shape 38x38 instead of 37x37\n",
    "base_model.features[max_pool_pos[-3]].ceil_mode = True\n",
    "# extract the model at conv4_3\n",
    "conv4_3_features = nn.Sequential(*base_model.features[:max_pool_pos[-2]])\n",
    "summary(conv4_3_features, (3,300,300), device = 'cpu')   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self, config, num_classes=21):\n",
    "        super(SSD, self).__init__()\n",
    "        # get hyper parameters from config file\n",
    "        self.aspect_ratios = config['model_params']['aspect_ratios']\n",
    "        self.scales = config['model_params']['scales']\n",
    "        self.scales.append(1.0)\n",
    "        self.no_boxes = config['model_params']['n_boxes']\n",
    "        self.channels = config['model_params']['channels']\n",
    "        self.num_classes = config['dataset_params']['num_classes']\n",
    "        self.iou_threshold = config['model_params']['iou_threshold']\n",
    "        self.low_score_threshold = config['model_params']['low_score_threshold']\n",
    "        self.neg_pos_ratio = config['model_params']['neg_pos_ratio']\n",
    "        self.pre_nms_topK = config['model_params']['pre_nms_topK']\n",
    "        self.nms_threshold = config['model_params']['nms_threshold']\n",
    "        self.detections_per_img = config['model_params']['detections_per_img']\n",
    "\n",
    "        # Load the pretrained VGG-16 model\n",
    "        base_model = torchvision.models.vgg16(\n",
    "            weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        # Extract all maxpool layers' index\n",
    "        max_pool_pos = [idx for idx, layer in enumerate(\n",
    "            list(base_model.features)) if isinstance(layer, nn.MaxPool2d)]\n",
    "        # force the output of conv4_3 to have shape 38x38 instead of 37x37\n",
    "        base_model.features[max_pool_pos[-3]].ceil_mode = True\n",
    "        # extract the model at conv4_3\n",
    "        self.conv4_3_features = nn.Sequential(\n",
    "            *base_model.features[:max_pool_pos[-2]])  # output: N x 512 x 38 x 38\n",
    "\n",
    "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
    "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
    "        # there are 512 channels in conv4_3_feats\n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "\n",
    "        ###################################\n",
    "        # Conv5_3 = nn.Sequential(*base_model.features[max_pool_pos[-2]:-1]) : 512x19x19\n",
    "        # Conv for fc6 and fc 7 #\n",
    "        ###################################\n",
    "        convs_instead_fcs = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1,\n",
    "                         padding=1),  # N x 512 x 19 x 19\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3,\n",
    "                      padding=6, dilation=6),  # Conv6 (FC6) : N x 1024 x 19 x 19\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Conv7 (FC7) : N x 1024 x 19 x 19\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # initialize weights for convs_instead_fcs\n",
    "        for layer in convs_instead_fcs:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    torch.nn.init.constant_(layer.bias, 0.0)\n",
    "        # attach convs_instead_fcs and layers after conv4_3_features\n",
    "        self.conv7_features = nn.Sequential(  # inout is the output of the  self.conv4_3_features with the shape of  N x 512 x 38 x 38\n",
    "            *base_model.features[max_pool_pos[-2]:-1],\n",
    "            convs_instead_fcs,\n",
    "        )  # output:  N x 1024 x 19 x 19\n",
    "\n",
    "        ###################################\n",
    "        # additional convolutions on top of the VGG base\n",
    "        ###################################\n",
    "        # Modules to take from 19x19 to 10x10\n",
    "        self.conv8_2_features = nn.Sequential(\n",
    "            # stride = 1, by default\n",
    "            nn.Conv2d(1024, 256, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # dim. reduction because stride > 1\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Modules to take from 10x10 to 5x5\n",
    "        self.conv9_2_features = nn.Sequential(\n",
    "            # stride = 1, by default\n",
    "            nn.Conv2d(512, 128, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # dim. reduction because stride > 1\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Modules to take from 5x5 to 3x3\n",
    "        self.conv10_2_features = nn.Sequential(\n",
    "            # stride = 1, by default\n",
    "            nn.Conv2d(256, 128, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # dim. reduction because stride > 1\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Modules to take from 3x3 to 1x1\n",
    "        self.conv11_2_features = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Initialize weights for additional convolutions\n",
    "        for conv_layer in [self.conv8_2_features, self.conv9_2_features, self.conv10_2_features, self.conv11_2_features]:\n",
    "            for layer in conv_layer.modules():\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                    if layer.bias is not None:\n",
    "                        torch.nn.init.constant_(layer.bias, 0.0)\n",
    "        ###################################\n",
    "        #  Prediction layers\n",
    "        ###################################\n",
    "        # classification head\n",
    "        self.classification_head = nn.ModuleList()\n",
    "        # localization head\n",
    "        self.bounding_box_head = nn.ModuleList()\n",
    "\n",
    "        for input_channels, no_boxes in zip(self.channels, self.no_boxes):\n",
    "            self.classification_head.append(nn.Conv2d(\n",
    "                input_channels,  no_boxes * self.num_classes, kernel_size=3, padding=1))\n",
    "            self.bounding_box_head.append(\n",
    "                nn.Conv2d(input_channels, 4 * no_boxes, kernel_size=3, padding=1))\n",
    "\n",
    "            # initialize conv weights\n",
    "            for module in self.classification_head:\n",
    "                torch.nn. init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "            for module in self.bounding_box_head:\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "    def prediction(self, outputs):\n",
    "        # Classification and bbox regression for all feature maps\n",
    "        cls_logits = []\n",
    "        bbox_reg_deltas = []\n",
    "        for i, features in enumerate(outputs):\n",
    "            # Predict classes in localization boxes\n",
    "            cls_feat_i = self.classification_head[i](features)\n",
    "            # classification_head output from (batch_size, no_boxes * num_classes, H, W) to (N, no_boxes *HW, num_classes).\n",
    "            N, _, H, W = cls_feat_i.shape\n",
    "            # (batch_size, no_boxes, num_classes, H, W)\n",
    "            cls_feat_i = cls_feat_i.view(N, -1, self.num_classes, H, W)\n",
    "            # (batch_size, H, W, no_boxes, num_classes)\n",
    "            cls_feat_i = cls_feat_i.permute(0, 3, 4, 1, 2)\n",
    "            # (batch_size, no_boxes *HW, num_classes)\n",
    "            cls_feat_i = cls_feat_i.reshape(N, -1, self.num_classes)\n",
    "            cls_logits.append(cls_feat_i)\n",
    "\n",
    "            # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n",
    "            bbox_reg_feat_i = self.bounding_box_head[i](features)\n",
    "            # Permute bbox reg output from (batch_size, no_boxes * 4, H, W) to (batch_size, no_boxes*HW, 4).\n",
    "            N, _, H, W = bbox_reg_feat_i.shape\n",
    "            bbox_reg_feat_i = bbox_reg_feat_i.view(\n",
    "                N, -1, 4, H, W)  # (batch_size, no_boxes, 4, H, W)\n",
    "            bbox_reg_feat_i = bbox_reg_feat_i.permute(\n",
    "                0, 3, 4, 1, 2)  # (batch_size, H, W, no_boxes, 4)\n",
    "            bbox_reg_feat_i = bbox_reg_feat_i.reshape(\n",
    "                N, -1, 4)  # Size=(batch_size, no_boxes*HW, 4)\n",
    "            bbox_reg_deltas.append(bbox_reg_feat_i)\n",
    "\n",
    "        # Concat cls logits and bbox regression predictions for all feature maps\n",
    "        # (batch_size, 8732, num_classes)\n",
    "        classes_scores = torch.cat(cls_logits, dim=1)\n",
    "        locs = torch.cat(\n",
    "            bbox_reg_deltas, dim=1)  # (batch_size, 8732, 4)\n",
    "        return locs, classes_scores\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Args:\n",
    "            image (Tensor): a tensor of dimensions (N, 3, 300, 300)\n",
    "\n",
    "        Returns:\n",
    "            _type_: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        # Run VGG base network convolutions (lower level feature map generators)\n",
    "        conv4_3_features_out = self.conv4_3_features(image)  # (N, 512, 38, 38)\n",
    "        # Rescale conv4_3 after L2 norm\n",
    "        norm = conv4_3_features_out.pow(2).sum(\n",
    "            dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_features_out_out_scaled = conv4_3_features_out / \\\n",
    "            norm  # (N, 512, 38, 38)\n",
    "        conv4_3_features_out_scaled = conv4_3_features_out_out_scaled * \\\n",
    "            self.rescale_factors  # (N, 512, 38, 38)\n",
    "\n",
    "        conv7_features_out = self.conv7_features(\n",
    "            conv4_3_features_out)  # ( N, 1024, 19, 19)\n",
    "\n",
    "        conv8_2_features_out = self.conv8_2_features(conv7_features_out)\n",
    "\n",
    "        conv9_2_features_out = self.conv9_2_features(conv8_2_features_out)\n",
    "\n",
    "        conv10_2_features_out = self.conv10_2_features(conv9_2_features_out)\n",
    "\n",
    "        conv11_2_features_out = self.conv11_2_features(conv10_2_features_out)\n",
    "\n",
    "        # Feature maps for predictions\n",
    "        outputs = [\n",
    "            conv4_3_features_out_scaled,  # 38 x 38\n",
    "            conv7_features_out,  # 19 x 19\n",
    "            conv8_2_features_out,  # 10 x 10\n",
    "            conv9_2_features_out,  # 5 x 5\n",
    "            conv10_2_features_out,  # 3 x 3\n",
    "            conv11_2_features_out,   # 1 x 1\n",
    "        ]\n",
    "\n",
    "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
    "        locs, classes_scores = self.prediction(outputs)\n",
    "        # locs: (N, 8732, 4),   classes_scores :  (N, 8732, n_classes)\n",
    "        return locs, classes_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-based",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
